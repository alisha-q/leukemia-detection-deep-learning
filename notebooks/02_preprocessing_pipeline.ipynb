{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Leukemia Classification - Preprocessing Pipeline\n",
        "\n",
        "**Project:** Build image preprocessing functions for model training\n",
        "\n",
        "**Goal:** Create reusable functions for image preprocessing and data splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1Ô∏è‚É£ Setup: Import Libraries and Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install kagglehub -q\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import kagglehub\n",
        "\n",
        "print('‚úÖ Libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2Ô∏è‚É£ Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "print('üì• Downloading dataset...')\n",
        "path = kagglehub.dataset_download('andrewmvd/leukemia-classification')\n",
        "\n",
        "# Set up paths\n",
        "leukemia_path = os.path.join(path, 'C-NMC_Leukemia')\n",
        "train_path = os.path.join(leukemia_path, 'training_data')\n",
        "\n",
        "print(f'‚úÖ Dataset downloaded!')\n",
        "print(f'üìÅ Path: {path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3Ô∏è‚É£ Function 1: Resize Images to 224x224\n",
        "\n",
        "**Why 224x224?**\n",
        "- Standard input size for transfer learning models (VGG, ResNet, etc.)\n",
        "- Good balance between detail and computational efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resize_image(image_path, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Resize an image to target size.\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        target_size (tuple): Target dimensions (width, height)\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Resized image as numpy array\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = Image.open(image_path)\n",
        "    \n",
        "    # Resize\n",
        "    img_resized = img.resize(target_size, Image.LANCZOS)\n",
        "    \n",
        "    # Convert to numpy array\n",
        "    img_array = np.array(img_resized)\n",
        "    \n",
        "    return img_array\n",
        "\n",
        "# Test the function\n",
        "fold_0_path = os.path.join(train_path, 'fold_0')\n",
        "test_img_path = os.path.join(fold_0_path, 'hem', os.listdir(os.path.join(fold_0_path, 'hem'))[0])\n",
        "\n",
        "original = Image.open(test_img_path)\n",
        "resized = resize_image(test_img_path)\n",
        "\n",
        "print(f'‚úÖ Resize function created!')\n",
        "print(f'Original size: {original.size}')\n",
        "print(f'Resized size: {resized.shape[:2]}')\n",
        "print(f'Array shape: {resized.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4Ô∏è‚É£ Function 2: Normalize Pixel Values (0-1 Range)\n",
        "\n",
        "**Why normalize?**\n",
        "- Neural networks train better with normalized inputs\n",
        "- Converts pixel values from [0, 255] to [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_image(img_array):\n",
        "    \"\"\"\n",
        "    Normalize pixel values to 0-1 range.\n",
        "    \n",
        "    Args:\n",
        "        img_array (numpy.ndarray): Image array with values 0-255\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Normalized image with values 0-1\n",
        "    \"\"\"\n",
        "    # Convert to float and divide by 255\n",
        "    normalized = img_array.astype('float32') / 255.0\n",
        "    \n",
        "    return normalized\n",
        "\n",
        "# Test the function\n",
        "test_normalized = normalize_image(resized)\n",
        "\n",
        "print(f'‚úÖ Normalize function created!')\n",
        "print(f'Original range: [{resized.min()}, {resized.max()}]')\n",
        "print(f'Normalized range: [{test_normalized.min():.3f}, {test_normalized.max():.3f}]')\n",
        "print(f'Data type: {test_normalized.dtype}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5Ô∏è‚É£ Function 3: Complete Preprocessing Pipeline\n",
        "\n",
        "**Combines:** Resize + Normalize + Convert to Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Complete preprocessing: resize, normalize, and convert to array.\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        target_size (tuple): Target dimensions (width, height)\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Preprocessed image ready for model\n",
        "    \"\"\"\n",
        "    # Resize\n",
        "    img_resized = resize_image(image_path, target_size)\n",
        "    \n",
        "    # Normalize\n",
        "    img_normalized = normalize_image(img_resized)\n",
        "    \n",
        "    return img_normalized\n",
        "\n",
        "# Test the function\n",
        "processed = preprocess_image(test_img_path)\n",
        "\n",
        "print(f'‚úÖ Preprocessing function created!')\n",
        "print(f'Output shape: {processed.shape}')\n",
        "print(f'Value range: [{processed.min():.3f}, {processed.max():.3f}]')\n",
        "print(f'Data type: {processed.dtype}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6Ô∏è‚É£ Function 4: Batch Preprocessing\n",
        "\n",
        "**Process multiple images at once**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_batch(image_paths, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Preprocess a batch of images.\n",
        "    \n",
        "    Args:\n",
        "        image_paths (list): List of image file paths\n",
        "        target_size (tuple): Target dimensions\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Array of preprocessed images\n",
        "    \"\"\"\n",
        "    processed_images = []\n",
        "    \n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            img = preprocess_image(img_path, target_size)\n",
        "            processed_images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f'Error processing {img_path}: {e}')\n",
        "    \n",
        "    return np.array(processed_images)\n",
        "\n",
        "print('‚úÖ Batch preprocessing function created!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7Ô∏è‚É£ Function 5: Get All Image Paths with Labels\n",
        "\n",
        "**Collect all image paths and their corresponding labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_image_paths_and_labels(data_path, folds=['fold_0', 'fold_1', 'fold_2']):\n",
        "    \"\"\"\n",
        "    Get all image paths and labels from the dataset.\n",
        "    \n",
        "    Args:\n",
        "        data_path (str): Path to training_data folder\n",
        "        folds (list): List of fold names to include\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (image_paths, labels)\n",
        "            image_paths (list): List of all image file paths\n",
        "            labels (list): List of labels (0=normal, 1=leukemia)\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    \n",
        "    for fold in folds:\n",
        "        fold_path = os.path.join(data_path, fold)\n",
        "        \n",
        "        # Normal images (label = 0)\n",
        "        normal_folder = os.path.join(fold_path, 'hem')\n",
        "        for img_name in os.listdir(normal_folder):\n",
        "            image_paths.append(os.path.join(normal_folder, img_name))\n",
        "            labels.append(0)\n",
        "        \n",
        "        # Leukemia images (label = 1)\n",
        "        leukemia_folder = os.path.join(fold_path, 'all')\n",
        "        for img_name in os.listdir(leukemia_folder):\n",
        "            image_paths.append(os.path.join(leukemia_folder, img_name))\n",
        "            labels.append(1)\n",
        "    \n",
        "    return image_paths, labels\n",
        "\n",
        "# Test the function\n",
        "all_paths, all_labels = get_image_paths_and_labels(train_path)\n",
        "\n",
        "print(f'‚úÖ Image paths collection function created!')\n",
        "print(f'Total images: {len(all_paths)}')\n",
        "print(f'Normal images: {all_labels.count(0)}')\n",
        "print(f'Leukemia images: {all_labels.count(1)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8Ô∏è‚É£ Function 6: Split Dataset (70:15:15)\n",
        "\n",
        "**Split data into:**\n",
        "- Training: 70%\n",
        "- Validation: 15%\n",
        "- Test: 15%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(image_paths, labels, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Split dataset into train, validation, and test sets.\n",
        "    \n",
        "    Args:\n",
        "        image_paths (list): List of image paths\n",
        "        labels (list): List of labels\n",
        "        train_ratio (float): Proportion for training (default 0.7)\n",
        "        val_ratio (float): Proportion for validation (default 0.15)\n",
        "        test_ratio (float): Proportion for test (default 0.15)\n",
        "        random_state (int): Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary with keys 'train', 'val', 'test'\n",
        "              Each contains tuple of (paths, labels)\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(image_paths)\n",
        "    y = np.array(labels)\n",
        "    \n",
        "    # First split: separate test set (15%)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_ratio, random_state=random_state, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Second split: separate train and validation from remaining 85%\n",
        "    # val_ratio adjusted: 15% of total = 15/85 of remaining\n",
        "    val_ratio_adjusted = val_ratio / (train_ratio + val_ratio)\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_ratio_adjusted, random_state=random_state, stratify=y_temp\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': (X_train.tolist(), y_train.tolist()),\n",
        "        'val': (X_val.tolist(), y_val.tolist()),\n",
        "        'test': (X_test.tolist(), y_test.tolist())\n",
        "    }\n",
        "\n",
        "# Test the function\n",
        "split_data = split_dataset(all_paths, all_labels)\n",
        "\n",
        "print(f'‚úÖ Dataset split function created!')\n",
        "print(f'\\nüìä Split Statistics:')\n",
        "print(f\"{'Set':<12} {'Total':<10} {'Normal':<10} {'Leukemia':<10} {'Percentage':<12}\")\n",
        "print('-' * 60)\n",
        "\n",
        "total_images = len(all_paths)\n",
        "for set_name in ['train', 'val', 'test']:\n",
        "    paths, labels = split_data[set_name]\n",
        "    normal_count = labels.count(0)\n",
        "    leukemia_count = labels.count(1)\n",
        "    percentage = (len(paths) / total_images) * 100\n",
        "    print(f\"{set_name.capitalize():<12} {len(paths):<10} {normal_count:<10} {leukemia_count:<10} {percentage:.1f}%\")\n",
        "\n",
        "print('-' * 60)\n",
        "print(f\"{'Total':<12} {total_images:<10} {all_labels.count(0):<10} {all_labels.count(1):<10} 100.0%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9Ô∏è‚É£ Test Preprocessing on 10 Images\n",
        "\n",
        "**Visualize original vs preprocessed images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 10 sample images (5 normal + 5 leukemia)\n",
        "train_paths, train_labels = split_data['train']\n",
        "\n",
        "# Get 5 normal and 5 leukemia images\n",
        "normal_samples = [p for p, l in zip(train_paths[:100], train_labels[:100]) if l == 0][:5]\n",
        "leukemia_samples = [p for p, l in zip(train_paths[:100], train_labels[:100]) if l == 1][:5]\n",
        "\n",
        "sample_paths = normal_samples + leukemia_samples\n",
        "sample_labels = [0]*5 + [1]*5\n",
        "\n",
        "# Preprocess the samples\n",
        "processed_samples = preprocess_batch(sample_paths)\n",
        "\n",
        "print(f'‚úÖ Processed {len(processed_samples)} images')\n",
        "print(f'Batch shape: {processed_samples.shape}')\n",
        "print(f'Value range: [{processed_samples.min():.3f}, {processed_samples.max():.3f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üîü Visualize Original vs Preprocessed\n",
        "\n",
        "**Compare before and after preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display original vs preprocessed\n",
        "fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
        "\n",
        "for i in range(10):\n",
        "    # Original image\n",
        "    original = Image.open(sample_paths[i])\n",
        "    axes[i//5*2, i%5].imshow(original)\n",
        "    label_text = 'Normal' if sample_labels[i] == 0 else 'Leukemia'\n",
        "    color = 'green' if sample_labels[i] == 0 else 'red'\n",
        "    axes[i//5*2, i%5].set_title(f'Original: {label_text}', color=color, fontsize=10)\n",
        "    axes[i//5*2, i%5].axis('off')\n",
        "    \n",
        "    # Preprocessed image\n",
        "    axes[i//5*2+1, i%5].imshow(processed_samples[i])\n",
        "    axes[i//5*2+1, i%5].set_title(f'Preprocessed (224x224)', fontsize=10)\n",
        "    axes[i//5*2+1, i%5].axis('off')\n",
        "\n",
        "plt.suptitle('Original vs Preprocessed Images', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('‚úÖ Visualization complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1Ô∏è‚É£1Ô∏è‚É£ Summary: All Preprocessing Functions\n",
        "\n",
        "### ‚úÖ Functions Created:\n",
        "\n",
        "1. **`resize_image(image_path, target_size=(224, 224))`**\n",
        "   - Resizes image to target dimensions\n",
        "\n",
        "2. **`normalize_image(img_array)`**\n",
        "   - Normalizes pixel values to [0, 1]\n",
        "\n",
        "3. **`preprocess_image(image_path, target_size=(224, 224))`**\n",
        "   - Complete preprocessing: resize + normalize\n",
        "\n",
        "4. **`preprocess_batch(image_paths, target_size=(224, 224))`**\n",
        "   - Process multiple images at once\n",
        "\n",
        "5. **`get_image_paths_and_labels(data_path, folds)`**\n",
        "   - Collect all image paths with labels\n",
        "\n",
        "6. **`split_dataset(image_paths, labels, train_ratio, val_ratio, test_ratio)`**\n",
        "   - Split data into train/val/test sets (70:15:15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1Ô∏è‚É£2Ô∏è‚É£ Complete Pipeline Test\n",
        "\n",
        "**Run the entire pipeline end-to-end**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('üîÑ Running Complete Preprocessing Pipeline...\\n')\n",
        "\n",
        "# Step 1: Get all image paths and labels\n",
        "print('Step 1: Collecting image paths...')\n",
        "image_paths, labels = get_image_paths_and_labels(train_path)\n",
        "print(f'   ‚úÖ Found {len(image_paths)} images')\n",
        "\n",
        "# Step 2: Split dataset\n",
        "print('\\nStep 2: Splitting dataset (70:15:15)...')\n",
        "data_splits = split_dataset(image_paths, labels)\n",
        "print(f'   ‚úÖ Train: {len(data_splits[\"train\"][0])} images')\n",
        "print(f'   ‚úÖ Val: {len(data_splits[\"val\"][0])} images')\n",
        "print(f'   ‚úÖ Test: {len(data_splits[\"test\"][0])} images')\n",
        "\n",
        "# Step 3: Preprocess a batch\n",
        "print('\\nStep 3: Preprocessing sample batch...')\n",
        "sample_batch_paths = data_splits['train'][0][:20]\n",
        "processed_batch = preprocess_batch(sample_batch_paths)\n",
        "print(f'   ‚úÖ Processed batch shape: {processed_batch.shape}')\n",
        "print(f'   ‚úÖ Value range: [{processed_batch.min():.3f}, {processed_batch.max():.3f}]')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('‚úÖ PREPROCESSING PIPELINE COMPLETE!')\n",
        "print('='*60)\n",
        "print('\\nüìä Pipeline Summary:')\n",
        "print(f'   ‚Ä¢ Total images: {len(image_paths)}')\n",
        "print(f'   ‚Ä¢ Target size: 224x224')\n",
        "print(f'   ‚Ä¢ Normalization: [0, 1]')\n",
        "print(f'   ‚Ä¢ Train/Val/Test: 70/15/15')\n",
        "print(f'   ‚Ä¢ Ready for model training! üöÄ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1Ô∏è‚É£3Ô∏è‚É£ Save Functions for Reuse\n",
        "\n",
        "**Save preprocessing functions to a Python file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create preprocessing.py file\n",
        "preprocessing_code = '''\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def resize_image(image_path, target_size=(224, 224)):\n",
        "    img = Image.open(image_path)\n",
        "    img_resized = img.resize(target_size, Image.LANCZOS)\n",
        "    return np.array(img_resized)\n",
        "\n",
        "def normalize_image(img_array):\n",
        "    return img_array.astype('float32') / 255.0\n",
        "\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    img_resized = resize_image(image_path, target_size)\n",
        "    img_normalized = normalize_image(img_resized)\n",
        "    return img_normalized\n",
        "\n",
        "def preprocess_batch(image_paths, target_size=(224, 224)):\n",
        "    processed_images = []\n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            img = preprocess_image(img_path, target_size)\n",
        "            processed_images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f'Error processing {img_path}: {e}')\n",
        "    return np.array(processed_images)\n",
        "\n",
        "def get_image_paths_and_labels(data_path, folds=['fold_0', 'fold_1', 'fold_2']):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for fold in folds:\n",
        "        fold_path = os.path.join(data_path, fold)\n",
        "        normal_folder = os.path.join(fold_path, 'hem')\n",
        "        for img_name in os.listdir(normal_folder):\n",
        "            image_paths.append(os.path.join(normal_folder, img_name))\n",
        "            labels.append(0)\n",
        "        leukemia_folder = os.path.join(fold_path, 'all')\n",
        "        for img_name in os.listdir(leukemia_folder):\n",
        "            image_paths.append(os.path.join(leukemia_folder, img_name))\n",
        "            labels.append(1)\n",
        "    return image_paths, labels\n",
        "\n",
        "def split_dataset(image_paths, labels, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
        "    X = np.array(image_paths)\n",
        "    y = np.array(labels)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_ratio, random_state=random_state, stratify=y\n",
        "    )\n",
        "    val_ratio_adjusted = val_ratio / (train_ratio + val_ratio)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_ratio_adjusted, random_state=random_state, stratify=y_temp\n",
        "    )\n",
        "    return {\n",
        "        'train': (X_train.tolist(), y_train.tolist()),\n",
        "        'val': (X_val.tolist(), y_val.tolist()),\n",
        "        'test': (X_test.tolist(), y_test.tolist())\n",
        "    }\n",
        "'''\n",
        "\n",
        "# Save to file\n",
        "with open('preprocessing.py', 'w') as f:\n",
        "    f.write(preprocessing_code)\n",
        "\n",
        "print('‚úÖ Preprocessing functions saved to preprocessing.py')\n",
        "print('\\nüí° You can now import these functions in other notebooks:')\n",
        "print('   from preprocessing import preprocess_image, split_dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ‚úÖ Checkpoint Complete!\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "1. ‚úÖ **Created resize function** (224x224)\n",
        "2. ‚úÖ **Created normalize function** (0-1 range)\n",
        "3. ‚úÖ **Created complete preprocessing pipeline**\n",
        "4. ‚úÖ **Created batch processing function**\n",
        "5. ‚úÖ **Created data splitting function** (70:15:15)\n",
        "6. ‚úÖ **Tested on 10 images**\n",
        "7. ‚úÖ **Saved functions for reuse**\n",
        "\n",
        "### üéØ Next Steps:\n",
        "\n",
        "- Build CNN model architecture\n",
        "- Create data generators for efficient training\n",
        "- Train the model\n",
        "- Evaluate performance\n",
        "\n",
        "---\n",
        "\n",
        "**üìù Remember to save this notebook to GitHub!**\n",
        "\n",
        "**How to save to GitHub:**\n",
        "1. File ‚Üí Download ‚Üí Download .ipynb\n",
        "2. Go to your GitHub repository\n",
        "3. Upload the file\n",
        "4. Commit with message: \"Added preprocessing pipeline notebook\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
